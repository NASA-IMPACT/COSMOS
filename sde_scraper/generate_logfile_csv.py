import csv
import json
from urllib.parse import urlparse


def parse_url(url):
    """takes a url and splits it into parts so it can be sorted by the path and protocol"""
    parsed_url = urlparse(url)
    if parsed_url.query:
        url_ending = f"{parsed_url.path}?{parsed_url.query}"
    else:
        url_ending = parsed_url.path

    return {
        # "url": url, # full url is already in source data
        "no_scheme_url": f"{parsed_url.netloc}/{url_ending}",
        "folder": "/".join(parsed_url.path.split("/")[:-1]),
        "file": parsed_url.path.split("/")[-1],
        "params": parsed_url.query,
        "protocol": parsed_url.scheme,
    }


def load_logfile(logfile_name):
    """opens the .log file and converts it to a list of dict entries"""
    logfile_path = f"raw_logfiles/{logfile_name}.log"

    with open(logfile_path) as f:
        logfile = f.readlines()
    logfile = [json.loads(entry) for entry in logfile]

    return logfile


def process_logfile(logfile):
    """reads through the urls and splits them into parts then sorts them"""

    # process_urls
    supplemented_logfile = []
    for entry in logfile:
        supplemented_logfile.append(
            {
                **parse_url(entry["url"]),
                **entry,
            }
        )

    # remove protocol variations
    # scraper gets the http and https for each link, this removes one
    no_variations = {}
    for entry in supplemented_logfile:
        no_variations[entry["no_scheme_url"]] = entry
    logfile = [value for key, value in no_variations.items()]

    # sort logfile on url
    logfile = sorted(logfile, key=lambda x: x["no_scheme_url"])

    # create csvable format
    pre_csv = []
    for entry in logfile:
        pre_csv.append(
            [
                entry["url"],
                entry["folder"],
                entry["file"],
                entry["params"],
                entry["title"],
            ]
        )

    headers = [
        "Full Url",
        "Folder",
        "File",
        "Params",
        "Scraped Title",
        "Index?",
        "How to Exclude",
        "Exclude Rationale",
        "Custom Title",
        "Custom Tree Path",
        "Custom Doc Type",
        "SME Advice",
    ]

    return [headers, *pre_csv]


def save_csv(processed_log_file, logfile_name):
    with open(f"processed_csvs/{logfile_name}.csv", "w") as csv_file:
        writer = csv.writer(csv_file, delimiter=",")
        for line in processed_log_file:
            writer.writerow(line)


def process_logfile_and_generate_csv(logfile_name):
    """
    takes a logfile generated by a spider, processes it and saves it to a csv
    the intention is that this csv is uploaded to google drive and used for indexing analysis
    """
    logfile = load_logfile(logfile_name)
    processed_log_file = process_logfile(logfile)
    save_csv(processed_log_file, logfile_name)
